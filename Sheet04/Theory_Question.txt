Cross entropy loss aims to maximize the MLE (maximum likelihood estimation), i.e, it tries to find the classifier paramaters that are most likely given training data. This is also equivalent to maximizing the maximum log likelihood, which is how we get the sum and log funcion in the loss function. Since we usually try to minimize the loss function, not maximize it, the negative sign as added, so that the minimization leads to maximization of MLE. 